<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://pkufjh.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://pkufjh.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-24T05:44:14+00:00</updated><id>https://pkufjh.github.io//feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">String method</title><link href="https://pkufjh.github.io//blog/2024/string-method/" rel="alternate" type="text/html" title="String method"/><published>2024-07-18T00:00:00+00:00</published><updated>2024-07-18T00:00:00+00:00</updated><id>https://pkufjh.github.io//blog/2024/string-method</id><content type="html" xml:base="https://pkufjh.github.io//blog/2024/string-method/"><![CDATA[<h1 id="minimum-free-energy-path-mfep">Minimum free energy path (MFEP)</h1> <p>Consider a system in the NVT ensemble, the equilibirum distribution is given by:</p> \[\begin{equation} \rho(x)=Z^{-1} e^{-\beta V(x)} \end{equation}\] <p>Suppose that one introduces N collective variables,</p> \[\begin{equation} \theta(x)=\left(\theta_1(x), \ldots, \theta_N(x)\right) \end{equation}\] <p>then the free energy associated with the collective variables is:</p> \[\begin{equation} \begin{aligned} F(z)= -k_B T \ln \left(Z^{-1} \int_{\mathbb{R}^n} e^{-\beta V(x)} \times \delta\left(z_1-\theta_1(x)\right) \cdots \delta\left(z_N-\theta_N(x)\right) d x\right) . \end{aligned} \end{equation}\] <p>Let us represent the MEP by the curve \(x(\alpha)\), where \(\alpha \in [0,1]\) is the parameter used to parametrize the curve. Since by definition the force \(- \nabla V\) must be everywhere tangent to the MEP, we must have</p> \[\begin{equation} \label{eq:1} \frac{d x_k(\alpha)}{d \alpha} \text { parallel to } \frac{\partial V(x(\alpha))}{\partial x_k} \text {. } \end{equation}\] <p>If the MEP connects the two minima of \(V(x)\) located at \(x_a\) and \(x_b\), Eq. \ref{eq:1} must be supplemented by the boundary conditions \(x(0)=x_a\) and \(x(1)=x_b\).</p> <p>Let us now consider things in the space of \(z = \theta(x)\), Eq. \ref{eq:1} implies that:</p> \[\begin{equation} \label{eq:2} \begin{aligned} \frac{d z_i(\alpha)}{d \alpha}= &amp; \sum_{k=1}^n \frac{\partial \theta_i(x(\alpha))}{\partial x_k} \frac{d x_k(\alpha)}{d \alpha} \\ &amp; \text { parallel to } \sum_{k=1}^n \frac{\partial \theta_i(x(\alpha))}{\partial x_k} \frac{\partial V(x(\alpha))}{\partial x_k} \\ = &amp; \sum_{j, k=1}^n \frac{\partial \theta_i(x(\alpha))}{\partial x_k} \frac{\partial \theta_j(x(\alpha))}{\partial x_k} \frac{\partial U(z(\alpha))}{\partial z_j} . \end{aligned} \end{equation}\] <p>It is shown in later section that it amounts to replacing \(U(z)\) by \(F(z)\) and the tensor \(\sum_k (\partial \theta_i / \partial x_k) \times (\partial \theta_j / \partial x_k)\) by its average \(M_{ij}\) defined as:</p> \[\begin{equation} \begin{aligned} M_{i j}(z)= &amp; Z^{-1} e^{\beta F(z)} \int_{\mathbb{R}^n} \sum_{k=1}^n \frac{\partial \theta_i(x)}{\partial x_k} \frac{\partial \theta_j(x)}{\partial x_k} e^{-\beta V(x)} \\ &amp; \times \delta\left(z_1-\theta_1(x)\right) \cdots \delta\left(z_N-\theta_N(x)\right) d x . \end{aligned} \end{equation}\] <p>Making these substutions into Eq. \ref{eq:2}, we arrive at:</p> \[\begin{equation} \frac{d z_i(\alpha)}{d \alpha} \text { parallel to } \sum_{j=1}^N M_{i j}(z(\alpha)) \frac{\partial F(z(\alpha))}{\partial z_j} \text {. } \end{equation}\] <p>which also means:</p> \[\begin{equation} 0=\sum_{j, k=1}^N P_{i j}(\alpha) M_{j k}(z(\alpha)) \frac{\partial F(z(\alpha))}{\partial z_k} \end{equation}\] <p>where \(P_{i j}\) is the projector on the plane perpendicular to the path at \(z(\alpha)\),</p> \[\begin{equation} \label{eq:3} P_{i j}(\alpha)=\delta_{i j}-\hat{t}_i(\alpha) \hat{t}_j(\alpha), \quad \hat{t}_i(\alpha)=\frac{\partial z_i / \partial \alpha}{\left|\partial z_i / \partial \alpha\right|} \end{equation}\] <p>How to find this solution in practice is explained in the next section.</p> <h1 id="string-method-and-mean-force-calculation">String method and mean force calculation</h1> <p>To solve Eq. \ref{eq:3}, The simplest way to evolve \(z(\alpha, t)\) so that it converges to a MFEP is to use,</p> \[\begin{equation} \frac{\partial z_i(\alpha, t)}{\partial t}=-\sum_{j, k=1}^N P_{i j}(\alpha, t) M_{j k}(z(\alpha, t)) \frac{\partial F(z(\alpha, t))}{\partial z_k}, \end{equation}\] <p>In practice, the evolution of the string toward the MFEP can be simulated as follows. The string is discretized using R discretization points (or images), for instance, as \(z^m (t) =z(m\Delta{\alpha}, t)\) with \(\Delta{\alpha} = 1/(R-1)\). To update the discretized string, we then proceed in four steps:</p> <ol> <li> <p>Calculation of the mean force \(\nabla_z F(z)\) and the tensor \(M(z)\),</p> </li> <li> <p>evolution of the string,</p> </li> <li> <p>smoothing of the string,</p> </li> <li> <p>reparametrization of the string.</p> </li> </ol> <p>The step 3 and 4 are due to statistical errors and numerical stability issues.</p>]]></content><author><name></name></author><category term="Physics"/><summary type="html"><![CDATA[This post describes string method.]]></summary></entry><entry><title type="html">Guided diffusion models</title><link href="https://pkufjh.github.io//blog/2024/guided-diffusion/" rel="alternate" type="text/html" title="Guided diffusion models"/><published>2024-07-03T00:00:00+00:00</published><updated>2024-07-03T00:00:00+00:00</updated><id>https://pkufjh.github.io//blog/2024/guided-diffusion</id><content type="html" xml:base="https://pkufjh.github.io//blog/2024/guided-diffusion/"><![CDATA[<h1 id="motivations">Motivations</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/guided-diffusion-480.webp 480w,/assets/img/guided-diffusion-800.webp 800w,/assets/img/guided-diffusion-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/guided-diffusion.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Generation of dogs through guided diffusion, with increasing amounts of guidance from left to right. </div> <p>Just as we generate wanted dogs pictures with learned guidance in the above figure, in generating molecule structures we also want to incorporate the physical knowledge, in order to generate structures whose distribution are closer to the true Boltzmann distribution. This can be done by the “energy-guided diffusion models” which will be introduced later.</p> <h1 id="diffusion-model-review">Diffusion model review</h1> <p>Forward process:</p> \[\begin{equation} q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)=\mathcal{N}\left(\mathbf{x}_t ; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right) \quad q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)=\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right) \end{equation}\] <p>Backward process:</p> \[\begin{equation} q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)=q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0\right) \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_0\right)}{q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)} \end{equation}\] <p>The forward process is a determined process in diffusion models, it is only a matter of learning the conditional probability function \(q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)\) or the so-called score function \(\nabla_{x_{t-1}} \log q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)\) in order to learn the data distribution and to do the sampling.</p> <h1 id="classifier-guidance">Classifier guidance</h1> <p>In the framework of classifier guidance, a classifier function should be learned to be used as guidance, namely the new score function goes like:</p> \[\begin{equation} \nabla_{x_t} \log p(x_t | y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y | x_t) \end{equation}\] <p>where \(\nabla_{x_t} \log p(x_t)\) is the original score function, \(\nabla_{x_t} \log p(y \mid x_t)\) is the learned classifier function term.</p> <p>In order to derive this formulation, A naive derivation goes like this:</p> \[\begin{equation} p(x_t | y) = \frac{p(x_t, y)}{p(y)} = \frac{p(y | x_t) p(x_t)}{p(y)} \end{equation}\] <p>where the new probability function now depends on the guidance label \(y\). And since the denominator does not depend on \(x_t\), we can reach the above formulation after taking logarithm and derivative.</p> <h1 id="classifier-free-guidance">Classifier-free guidance</h1> <p>Let’s add the scale factor to the classifier guidance score function:</p> \[\begin{equation} \nabla_{x_t} \log p(x_t) + \eta \nabla_{x_t} \log p(y | x_t) \end{equation}\] <p>Now we again use the bayes’ rule:</p> \[\begin{equation} p(y | x_t) = p(x_t | y) \frac{p(y)}{p(x_t)} \end{equation}\] <p>So the score function can be rewritten as:</p> \[\begin{equation} \begin{aligned} s(x_t, x_{t+1}, y) &amp; = \nabla_{x_t} \log p(x_t) + \eta \nabla_{x_t} (\log p(x_t | y) -\log p(x_t) )\\ &amp; = (1-\eta)\nabla_{x_t} \log p(x_t) + \eta \nabla_{x_t} \log p(x_t | y) \end{aligned} \end{equation}\] <p>Namely, we can train one model conditioned on the label selectively. When doing inference, we can use \(\eta\) to control the scale of the conditioned score function.</p> <h1 id="energy-guided-diffusion">Energy guided diffusion</h1> <p>The target distribution is:</p> \[\begin{equation} p_0\left(\boldsymbol{x}_0\right) \propto q_0\left(\boldsymbol{x}_0\right) e^{-\beta \mathcal{E}\left(\boldsymbol{x}_0\right)} . \end{equation}\] <p>Suppose the marginal distribution \(q_t (x_t)\) of the forward process starting from \(q_0(x_0)\), then a pre-trained model can be obtained by learning the the score function \(\nabla_{x_t} \log q_t(x_t)\). In the same flavor, in order to learn \(p_0 (x_0)\), the model have to learn the intermediate score function \(\nabla_{x_t} \log p_t(x_t)\). In the following we show that under the assumption that the noising process for \(p_0(x_0)\) and \(q_0(x_0)\) are the same, the noisy distribution \(p_t(x_t)\) and \(q_t(x_t)\) will satisfy:</p> \[\begin{equation} p_t\left(\boldsymbol{x}_t\right) \propto q_t\left(\boldsymbol{x}_t\right) e^{-\mathcal{E}_t\left(\boldsymbol{x}_t\right)}, \end{equation}\] <p>where：</p> \[\begin{equation} \mathcal{E}_t\left(\boldsymbol{x}_t\right):= \begin{cases}\beta \mathcal{E}\left(\boldsymbol{x}_0\right), &amp; t=0 \\ -\log \mathbb{E}_{q_{0 t}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_t\right)}\left[e^{-\beta \mathcal{E}\left(\boldsymbol{x}_0\right)}\right], &amp; t&gt;0\end{cases} \end{equation}\] <p>We give a proof here:</p> \[\begin{equation} p_{t 0}\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right):=q_{t 0}\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)=\mathcal{N}\left(\boldsymbol{x}_t \mid \alpha_t \boldsymbol{x}_0, \sigma_t^2 \boldsymbol{I}\right) \end{equation}\] \[\begin{equation} \begin{aligned} p_t\left(\boldsymbol{x}_t\right) &amp; =\int p_{t 0}\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right) p_0\left(\boldsymbol{x}_0\right) \mathrm{d} \boldsymbol{x}_0 \\ &amp; =\int p_{t 0}\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right) q_0\left(\boldsymbol{x}_0\right) \frac{e^{-\beta \mathcal{E}\left(\boldsymbol{x}_0\right)}}{Z} \mathrm{~d} \boldsymbol{x}_0 \\ &amp; =\int q_{t 0}\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right) q_0\left(\boldsymbol{x}_0\right) \frac{e^{-\beta \mathcal{E}\left(\boldsymbol{x}_0\right)}}{Z} \mathrm{~d} \boldsymbol{x}_0 \\ &amp; =q_t\left(\boldsymbol{x}_t\right) \int q_{0t}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_t\right) \frac{e^{-\beta \mathcal{E}\left(\boldsymbol{x}_0\right)}}{Z} \mathrm{~d} \boldsymbol{x}_0 \\ &amp; =\frac{q_t\left(\boldsymbol{x}_t\right) \mathbb{E}_{q_{0t}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_t\right)}\left[e^{\left.-\beta \mathcal{E}\left(\boldsymbol{x}_0\right)\right]}\right.}{Z} \\ &amp; =\frac{q_t\left(\boldsymbol{x}_t\right) e^{-\mathcal{E}_t\left(\boldsymbol{x}_t\right)}}{Z} \end{aligned} \end{equation}\] <p>The score function with energy guidance is:</p> \[\begin{equation} \nabla_{\boldsymbol{x}_t} \log p_t\left(\boldsymbol{x}_t\right)=\underbrace{\nabla_{\boldsymbol{x}_t} \log q_t\left(\boldsymbol{x}_t\right)}_{\approx-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_t, t\right) / \sigma_t}-\underbrace{\nabla_{\boldsymbol{x}_t} \mathcal{E}_t\left(\boldsymbol{x}_t\right)}_{\begin{array}{c} \text {energy guidance (intractable)} \end{array}} . \end{equation}\] <p>The first term is given by the pretrained model, the remaining problem is to estimate the latter. We show in the next slide that the solution \(f_{\boldsymbol{\phi}^*}\left(\boldsymbol{x}_t, t\right)\) to the following problem:</p> \[\begin{equation} \min _\phi \mathbb{E}_{p(t)} \mathbb{E}_{q_0\left(\boldsymbol{x}_0^{(1: K)}\right)} \mathbb{E}_{p\left(\boldsymbol{\epsilon}^{(1: K)}\right)}\left[-\sum_{i=1}^K e^{-\beta \mathcal{E}\left(\boldsymbol{x}_0^{(i)}\right)} \log \frac{e^{-f_\phi\left(\boldsymbol{x}_t^{(i)}, t\right)}}{\sum_{j=1}^K e^{-f_\phi\left(\boldsymbol{x}_t^{(j)}, t\right)}}\right] \end{equation}\] <p>will satisfy \(\nabla_{\boldsymbol{x}_t} f_{\boldsymbol{\phi}^*}\left(\boldsymbol{x}_t, t\right)=\nabla_{\boldsymbol{x}_t} \mathcal{E}_t\left(\boldsymbol{x}_t\right)\). Intuitively, it is the cross-entropy loss between the intermediate energy model and the trained model.</p> <h1 id="force-guided-diffusion">Force guided diffusion</h1> <p>Similar to the intermediate energy expression, we can define the intermediate force as:</p> \[\begin{equation} \nabla_{\mathbf{x}_t} \mathcal{E}_t\left(\mathbf{x}_t\right)=\frac{\mathbb{E}_{q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right)}\left[e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)} \zeta\left(\mathbf{x}_0, \mathbf{x}_t\right)\right]}{k \mathbb{E}_{q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right)}\left[e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)}\right]} \end{equation}\] <p>where \(\zeta\left(\mathbf{x}_0, \mathbf{x}_t\right)=\nabla_{\mathbf{x}_t} \log q_t\left(\mathbf{x}_t\right)-\nabla_{\mathbf{x}_t} \log q_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\). We give a proof here, from the expression of the intermediate energy, we have the expression of the intermediate force</p> \[\begin{equation} \nabla_{\mathbf{x}_t} \mathcal{E}_t\left(\mathbf{x}_t\right)=-\frac{\int e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)} \nabla_{\mathbf{x}_t} q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right) \mathrm{d} \mathbf{x}_0}{k \int q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right) e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)} \mathrm{d} \mathbf{x}_0} \end{equation}\] <p>The numerator \(N\) evaluates as:</p> \[\begin{equation} \begin{aligned} N &amp; =\int e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)} q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right) \nabla_{\mathbf{x}_t} \log q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right) \mathrm{d} \mathbf{x}_0 \\ &amp; =\int q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right) e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)} \nabla_{\mathbf{x}_t} \log \frac{q_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right) q_0\left(\mathbf{x}_0\right)}{q_t\left(\mathbf{x}_t\right)} \mathrm{d} \mathbf{x}_0 \\ &amp; =\int q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right) e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)}\left(\nabla_{\mathbf{x}_t} \log q_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)-\nabla_{\mathbf{x}_t} \log q_t\left(\mathbf{x}_t\right)\right) \mathrm{d} \mathbf{x}_0 \\ &amp; =\mathbb{E}_{q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right)}\left[e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)}\left(\nabla_{\mathbf{x}_t} \log q_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)-\nabla_{\mathbf{x}_t} \log q_t\left(\mathbf{x}_t\right)\right)\right] \end{aligned} \end{equation}\] <p>the intermediate force evaluates as:</p> \[\begin{equation} \begin{aligned} \nabla_{\mathbf{x}_t} \mathcal{E}_t\left(\mathbf{x}_t\right) &amp; =-\frac{\mathbb{E}_{q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right)}\left[-e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)} \zeta\left(\mathbf{x}_0, \mathbf{x}_t\right)\right]}{k \mathbb{E}_{q_t\left(\mathbf{x}_0 \mid \mathbf{x}_t\right)}\left[e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)}\right]} \\ &amp; =\frac{\mathbb{E}_{q_0\left(\mathbf{x}_0\right)}\left[q_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right) e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)} \zeta\left(\mathbf{x}_0, \mathbf{x}_t\right)\right]}{k \mathbb{E}_{q_0\left(\mathbf{x}_0\right)}\left[q_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right) e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)}\right]} . \end{aligned} \end{equation}\] <p>where \(\zeta\left(\mathbf{x}_0, \mathbf{x}_t\right)=\nabla_{\mathbf{x}_t} \log q_t\left(\mathbf{x}_t\right)-\nabla_{\mathbf{x}_t} \log q_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\). So the idea is that we can train a model to learn this intermediate force, and apply that to guide the diffusion process, namely the score function goes:</p> \[\begin{equation} \nabla_{\mathbf{x}_t} \log p_t\left(\mathbf{x}_t\right)=\nabla_{\mathbf{x}_t} \log q_t\left(\mathbf{x}_t\right)-\eta \nabla_{\mathbf{x}_t} \mathcal{E}_t\left(\mathbf{x}_t\right), \end{equation}\] <p>where \(\eta\) is the parameter which controls the guidance strength.</p> <p>Finally we have the force-guided algorithm:</p> \[\begin{equation} \begin{aligned} &amp;\text { Input: generated data } \mathbf{x}_0 \text { in a batch } K \text {, score model }\\ &amp;s_\theta\left(\mathbf{x}_t, t\right) \text { in Section 4.1, energy } \mathcal{E}_0\left(\mathbf{x}_0\right) \text {, force } \nabla_{\mathbf{x}_0} \mathcal{E}_0\left(\mathbf{x}_0\right) \text {, }\\ &amp;\text { intermediate force network } h_\psi\left(\mathbf{x}_t, t\right)\\ &amp;\text { for training iterations do }\\ &amp;t \sim \mathcal{U}(0,1) .\\ &amp;\mathbf{x}_t=\text { Forward diffusion}\\ &amp;q_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right) \sim \mathcal{N}\left(\mathbf{x}_t ; \sqrt{\alpha_t} \mathbf{x}_0,\left(1-\alpha_t\right) I\right)\\ &amp;\zeta\left(\mathbf{x}_0, \mathbf{x}_t\right)=s_\theta\left(\mathbf{x}_t, t\right)-\nabla \log q_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right) \text {. }\\ &amp;Y=\sum_{\mathbf{x}_0} q_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right) e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)} \text {. }\\ &amp;\mathcal{L}=\frac{1}{K} \sum\left\|h_\psi\left(\mathbf{x}_t, t\right)-e^{-k \mathcal{E}_0\left(\mathbf{x}_0\right)} \zeta\left(\mathbf{x}_0, \mathbf{x}_t\right) / Y\right\|_2^2 .\\ &amp;\min _\psi \mathcal{L} \text {. } \text{end for}. \end{aligned} \end{equation}\]]]></content><author><name></name></author><category term="AI"/><summary type="html"><![CDATA[This post describes diffusion models with guidance.]]></summary></entry><entry><title type="html">All you need to know about free energy (Part I)</title><link href="https://pkufjh.github.io//blog/2023/free-energy/" rel="alternate" type="text/html" title="All you need to know about free energy (Part I)"/><published>2023-11-07T00:00:00+00:00</published><updated>2023-11-07T00:00:00+00:00</updated><id>https://pkufjh.github.io//blog/2023/free-energy</id><content type="html" xml:base="https://pkufjh.github.io//blog/2023/free-energy/"><![CDATA[<h1 id="fundamental-theory-of-free-energy">Fundamental Theory of Free Energy</h1> <h2 id="what-is-free-energy">What is Free Energy</h2> <p>Thermodynamic perspective (at constant temperature and volume):</p> \[F = U - TS\] <p>The (Helmholtz) free energy \(F\) of a system consists of two parts: the internal energy \(U\) and the entropy \(S\) (degree of disorder) of the system. According to the second law of thermodynamics, an isolated system will always evolve towards a state of increased entropy. In a constant temperature and volume scenario, the second law of thermodynamics states that a system will evolve towards a state where the free energy \(F\) decreases. From the above formula, we can see that this trend is determined by two state quantities: 1. The system tends to evolve towards a state with lower internal energy \(U\). 2. The system tends to evolve towards a state with higher entropy \(S\). Due to the presence of the temperature \(T\) coefficient in front of the entropy term, we can immediately see that at higher temperatures, the system’s state change is more influenced by entropy, while at lower temperatures, it is more influenced by internal energy.</p> <p>Statistical mechanics perspective:</p> \[F = - \frac{1}{\beta} \ln Z, Z =\int \mathrm{d}^N \mathbf{r} \mathrm{e}^{-\beta E \left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)}\] <p>where \(\beta = \frac{1}{k_B T}\), \(Z\) is the partition function of the system. From the statistical mechanics definition of free energy \(F\), it is evident that the free energy of the system is entirely determined by the magnitude and distribution of energy \(E\) in the coordinate space. To reduce the free energy, the system will tend to adopt states with lower energy \(E\). Moreover, due to the presence of the integral sign, the system will tend to form states with a higher number of microstates at the same energy level, which corresponds to entropy in thermodynamics.</p> <h2 id="calculation-of-free-energy">Calculation of Free Energy</h2> <p>From the aforementioned definitions, a direct observation is that the free energy \(F\) of a system depends on the definition of internal energy \(U\). Since the definition of energy is relative, the absolute value of the system’s free energy \(F\) is also meaningless. Typically, we are almost always concerned with the change in free energy between two states of the system, namely:</p> \[\Delta F = F_B - F_A\] <p>According to the statistical mechanics definition of free energy, the change in free energy can be written as:</p> \[\Delta F = - k_B T \ln Z_B + k_B T \ln Z_A = - k_B T \ln \frac{Z_B}{Z_A}\] <p>where \(Z_A\) and \(Z_B\) are the partition functions of the system in states A and B, respectively:</p> \[\begin{aligned} Z_{\mathcal{A}} &amp; =\int \mathrm{d}^N \mathbf{r} \mathrm{e}^{-\beta U_{\mathcal{A}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)} \\ Z_{\mathcal{B}} &amp; =\int \mathrm{d}^N \mathbf{r} \mathrm{e}^{-\beta U_{\mathcal{B}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)} \end{aligned}\] <p>This concludes the introduction to the definition of free energy calculation. The following sections will delve into how to compute it, specifically focusing on how to use molecular dynamics simulations to calculate free energy, as this is a tutorial on molecular dynamics.</p> <p>If we know the form of the energy function \(U(r)\) for the system in states A and B, then calculating the partition function is straightforward. However, in general, we do not know this, which is one reason why we use molecular dynamics as a tool. For molecular dynamics, we usually cannot obtain the partition function for a system in a particular state. Instead, we can obtain the ensemble distribution (typically at constant temperature and volume) and ensemble average (time average) of a physical quantity in that state. Therefore, the core idea of using molecular dynamics to obtain the change in the system’s state free energy is to convert the free energy change into an ensemble average of some physical quantity, which is the starting point for most free energy calculation methods.</p> <p>Next, we will look at two main approaches to free energy calculation: equilibrium methods and non-equilibrium methods.</p> <h2 id="equilibrium-free-energy-calculation-methods">Equilibrium Free Energy Calculation Methods</h2> <h3 id="free-energy-perturbation-fep">Free Energy Perturbation (FEP)</h3> <p>To write the change in the system’s free energy as an ensemble average of some physical quantity, we note that the partition function part of the free energy change can be written as:</p> \[\begin{aligned} \frac{Z_{\mathcal{B}}}{Z_{\mathcal{A}}} &amp; =\frac{1}{Z_{\mathcal{A}}} \int \mathrm{d}^N \mathbf{r} \mathrm{e}^{-\beta U_{\mathcal{A}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)} \mathrm{e}^{-\beta\left(U_{\mathcal{B}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)-U_{\mathcal{A}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)\right)} \\ &amp; =\left\langle\mathrm{e}^{-\beta\left(U_{\mathcal{B}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)-U_{\mathcal{A}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)\right)}\right\rangle_{\mathcal{A}} \end{aligned}\] <p>Therefore, the change in free energy of the system becomes:</p> \[\Delta F_{\mathcal{A B}}=-k T \ln \left\langle\mathrm{e}^{-\beta\left(U_{\mathcal{B}}-U_{\mathcal{A}}\right)}\right\rangle_{\mathcal{A}}\] <p>From the perspective of molecular dynamics sampling, this formula can be interpreted as: we sample the potential energy surface of the system in state A, obtaining a series of configurations in the ensemble distribution of the system in state A. For each such configuration, we calculate \(\mathrm{e}^{-\beta\left(U_{\mathcal{B}}-U_{\mathcal{A}}\right)}\), and the average of these configurations gives the ensemble average of this physical quantity.</p> <p>Note that the definitions of potential energy in states A and B, \(U_A\) and \(U_B\), are different. Consequently, low-energy configurations in state A may not be low-energy configurations in state B. When states A and B differ significantly, the energy calculated in state B for configurations sampled in state A may be high-energy configurations, resulting in small weight factors \(\mathrm{e}^{-\beta\left(U_{\mathcal{B}}-U_{\mathcal{A}}\right)}\). Due to the limited sampling time in molecular dynamics, we cannot sample configurations with high weight factors, leading to an efficiency problem in our calculations. If states A and B differ significantly, it takes a very long time to obtain a convergent free energy difference estimate.</p> <p>To solve this sampling efficiency problem, our solution is: since free energy is an equilibrium state property, we can design a path from state A to state B and calculate the free energy difference between adjacent states along this path. Then, the free energy difference between states A and B can be expressed as:</p> \[\Delta F_{\mathcal{A B}}= \sum_{\alpha=1}^{M-1} F_{\alpha+1} - F_{\alpha} = -k T \sum_{\alpha=1}^{M-1} \ln \left\langle\mathrm{e}^{-\beta \Delta U_{\alpha, \alpha+1}}\right\rangle_\alpha\] <p>Since the differences between adjacent states on the path are relatively small, their free energy differences converge more quickly. We can sample all the states on the path simultaneously, obtaining a convergent free energy difference more quickly, which is the free energy perturbation method.</p> <h3 id="barmbar">BAR/MBAR</h3> <p>In the derivation of the free energy perturbation theory, we find that a crucial step is the transformation of the ratio of partition functions:</p> \[\begin{aligned} \frac{Z_{\mathcal{B}}}{Z_{\mathcal{A}}} &amp; =\frac{1}{Z_{\mathcal{A}}} \int \mathrm{d}^N \mathbf{r} \mathrm{e}^{-\beta U_{\mathcal{A}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)} \mathrm{e}^{-\beta\left(U_{\mathcal{B}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)-U_{\mathcal{A}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)\right)} \\ &amp; =\left\langle\mathrm{e}^{-\beta\left(U_{\mathcal{B}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)-U_{\ mathcal{A}}\left(\mathbf{r}_1, \ldots, \mathbf{r}_N\right)\right)}\right\rangle_{\mathcal{A}} \end{aligned}\] <p>This transformation is not unique. For example, we can adopt the following transformation:</p> \[\frac{Z_B}{Z_A}=\frac{Z_B}{Z_A} \frac{\int W e^{-U_B-U_A} d \mathbf{q}}{\int W e^{-U_B-U_A} d \mathbf{q}}=\frac{\left\langle W e^{-U_B}\right\rangle_A}{\left\langle W e^{-U_A}\right\rangle_B}\] <p>In this transformation, any choice of the function \(W\) is permissible. Specifically, if we choose \(W = e^{U_A}\), this transformation reverts to the equation in the aforementioned FEP method.</p> <p>Now, the question arises: what form of the \(W\) function achieves the optimal transformation? More precisely, what choice allows us to achieve the best convergence when performing molecular dynamics sampling using this transformation? To address this issue, the Bennett Acceptance Ratio (BAR) method was developed.</p> <p>BAR, which stands for Bennett Acceptance Ratio, is a method proposed by Bennett in 1976 for calculating free energy. The core of this method lies in selecting the form of the \(W\) function. For a detailed derivation, see <a href="https://dasher.wustl.edu/chem478/reading/jcompphys-22-245-76.pdf">Bennet 1976</a>. Below, we present the result directly. Suppose we use molecular dynamics simulations to sample the system in state A with a sample size of \(N_A\) and the system in state B with a sample size of \(N_B\). The optimal choice of the \(W\) function to minimize the variance of the partition function ratio in this sampling is:</p> \[W = \frac{1}{\frac{Z_B}{N_B} e^{-U_A}+\frac{Z_A}{N_A} e^{-U_B}}\] <p>Note that the \(W\) function depends on the values of \(Z_B\) and \(Z_A\), making the \(W\) function and the partition functions interdependent. In practice, a self-consistent iterative calculation is required.</p> <p>Using the BAR method, we can calculate the free energy difference between two states. Although the choice of the \(W\) function is optimal for the existing samples in both states, the problem of low sampling efficiency remains when states A and B differ significantly. Can we emulate FEP to calculate the free energy difference between multiple states? The Multistate Bennett Acceptance Ratio (MBAR) method is a multistate extension of the BAR method.</p> <p>In summary, the MBAR method addresses the following problem: given a system with \(K\) states, we have a series of samples for each of these \(K\) states, with the number of samples for each state denoted by \(N_i\). How can we optimally calculate the free energy difference between pairs of states, i.e.,</p> \[\Delta F_{i j}=-\beta^{-1} \ln \frac{Z_j}{Z_i}\] <p>Just like the BAR method, we have transformations for the ratio of partition functions:</p> \[Z_i\left\langle\alpha_{i j} \exp \left(-\beta U_j\right)\right\rangle_i=Z_j\left\langle\alpha_{i j} \exp \left(-\beta U_i\right)\right\rangle_j\] <p>The MBAR method concludes with the optimal selection of the coefficients \(\alpha_{i j}\):</p> \[\alpha_{i j}=\frac{N_j{\hat{z_j}}^{-1}}{\sum_{k=1}^K N_k{\hat{z_k}}^{-1} \exp \left(-\beta U_k\right)}\] <p>where \(\hat{z_i}\) is our current estimate of the partition functions based on the samples. This is still a self-consistent iterative calculation formula.</p> <h3 id="hamiltonian-thermodynamic-integration">Hamiltonian Thermodynamic Integration</h3> <p>We have introduced two common free energy calculation methods, and we can see that the MBAR method is an extension of the FEP method. Next, let us consider free energy calculation from a more general perspective.</p> <p>The two methods introduced earlier, FEP and MBAR, both require sampling multiple states of a system and calculating the potential energy functions. During this process, we need to note that the potential energy functions change for different states of the system. This is crucial, and it is the basis for performing free energy calculations. More precisely, for practical system calculations, the form of the potential energy function is:</p> \[U = U(\mathbf{r}, \lambda (\mathbf{r}))\] <p>Alternatively, different states of a system essentially have different Hamiltonians:</p> \[H = H(\mathbf{r}, \lambda (\mathbf{r}))\] <p>The free energy difference between different states, or the free energy of different states, is actually a function of the variable \(\lambda\) in this function. Specifically, if we calculate the binding free energy of a ligand and receptor, in the initial state A, where the ligand and receptor are separated, the Hamiltonian of the entire system does not include the interaction function between the ligand and receptor. In the final state B, where the ligand and receptor are bound, the Hamiltonian of the system includes the interaction function between the ligand and receptor. This interaction is gradually turned on.</p> <p>In the two methods introduced earlier, FEP and MBAR, the change in the Hamiltonian for different states of the system is implicit, achieved by changing the configuration of the system and implicitly changing the Hamiltonian using the built-in potential energy function of MD. In other words, \(\lambda\) here is an implicit variable. The free energy obtained for different states does not indicate which specific variable of the system it is a function of; we only know it is a variable related to the overall state of the system. Can we explicitly write out the change in the Hamiltonian for different states of the system? Of course, we can. Since free energy is a state function of the system, we can design a path to let the system evolve from state A to state B.</p> <p>We define the following series of Hamiltonians:</p> \[\mathcal{H}(\mathbf{x}, \lambda)=(1-\lambda) \mathcal{H}_A+\lambda \mathcal{H}_B\] <p>Since the system Hamiltonian explicitly contains the variable \(\lambda\), the system’s free energy naturally contains \(\lambda\):</p> \[F(N, V, T, \lambda)=-k_B T \ln Z(N, V, T, \lambda)\] <p>From this explicit expression, we can directly write the derivative of free energy with respect to the variable \(\lambda\):</p> \[\begin{aligned} \frac{\partial F}{\partial \lambda} &amp; =-k_B T \frac{\frac{\partial}{\partial \lambda} Z(N, V, T, \lambda)}{Z(N, V, T, \lambda)} \\ &amp; =-k_B T \frac{\int \exp [-\beta \mathcal{H}(\mathbf{x}, \lambda)] \cdot(-\beta) \frac{\partial \mathcal{H}}{\partial \lambda} \mathrm{d} \mathbf{x}}{\int \exp [-\beta \mathcal{H}(\mathbf{x}, \lambda)] \mathrm{d} \mathbf{x}} \\ &amp; =\left\langle\frac{\partial \mathcal{H}}{\partial \lambda}\right\rangle_\lambda \\ &amp; =\left\langle U_B-U_A\right\rangle_\lambda \end{aligned}\] <p>By definition of the integral, we naturally obtain the formula for the free energy difference:</p> \[\Delta F = F_B - F_A =\int_0^1\left\langle U_B-U_A\right\rangle_\lambda \mathrm{d} \lambda\] <p>In actual sampling, we use this formula by performing a series of samplings at different discrete \(\lambda\) values and calculating the corresponding \(U_B - U_A\), then summing them to obtain an approximate estimate of the free energy difference.</p> <h3 id="umbrella-sampling">Umbrella Sampling</h3> <p>From the discussion in the previous section on thermodynamic integration, we recognize that the starting point for free energy calculation is to express the change in the Hamiltonian (potential energy function) of the system either implicitly (FEP and MBAR) or explicitly (HI method) using some variable and estimate the free energy difference through sampling. With this understanding, let’s look at a new sampling method (which is also a free energy calculation method): umbrella sampling, which is another explicit method for expressing Hamiltonian changes.</p> <p>In umbrella sampling, we define a collective variable \(\xi(r)\) to explicitly express the change in the system state. It is assumed that coordinates unrelated to this collective variable have a negligible effect on the system state change. Therefore, the Hamiltonian of the system becomes a function of this collective variable:</p> \[H = H(\mathbf{r}, \xi (\mathbf{r}))\] <p>Similarly, just as we sample at different parameter \(\lambda\) values in the Hamiltonian integration method, in umbrella sampling, we sample extensively at different values of the collective variable \(\xi(r)\). Umbrella sampling achieves this by imposing a large restraint potential on the collective variable \(\xi(r)\):</p> \[\omega_i(\xi)=K / 2\left(\xi(r)-\xi_i^{\mathrm{ref}}\right)^2\] <p>The total potential energy</p> <p>function with the bias potential added is:</p> \[E^{\mathrm{b}}(r)=E^{\mathrm{u}}(r)+\omega_i(\xi)\] <p>We know that free energy is a property of the equilibrium state of the system, meaning that free energy should be determined by the equilibrium potential energy function of the system:</p> \[F_i (\xi) = - \frac{1}{\beta} \int \exp [-\beta E^{u}(r)] \delta\left[\xi(r)-\xi\right] d^{N}{r} =- \frac{1}{\beta} P_{i}^{\mathrm{u}}(\xi)\] <p>where \(P_{i}^{\mathrm{u}}(\xi)\) is the probability distribution of the collective variable \(\xi(r)\) in the equilibrium state of the system:</p> \[P_{i}^{\mathrm{u}}(\xi)=\frac{\int \exp [-\beta E(r)] \delta\left[\xi(r)-\xi\right] d^{N}{r}}{\int \exp [-\beta E(r)] d^{N}{r}}\] <p>On the other hand, in umbrella sampling, the system evolves under the total potential energy function \(E^{\mathrm{b}}(r)\) with the bias potential added. The real question in umbrella sampling is: how can we use the trajectories under the bias potential to calculate the equilibrium free energy? To achieve this, we try to use the bias potential to express the equilibrium free energy. First, we can write the probability distribution of the collective variable \(\xi(r)\) under the bias potential:</p> \[P*{i}^{\mathrm{b}}(\xi)=\frac{\int \exp \left\{-\beta\left[E(r)+\omega*{i}\left(\xi(r)\right)\right]\right\} \delta\left[\xi(r)-\xi\right] d^{N}{r}}{\int \exp \left\{-\beta\left[E(r)+\omega_{i}\left(\xi(r)\right)\right]\right\} d^{N}{r}}\] <p>Therefore, we can express the equilibrium probability distribution using the bias probability distribution:</p> \[\begin{aligned} P_{i}^{\mathrm{u}}(\xi)=&amp; P_{i}^{\mathrm{b}}(\xi) \exp \left[\beta \omega_{i}(\xi)\right] \\ &amp; \times \frac{\int \exp \left\{-\beta\left[E(r)+\omega_{i}(\xi(r))\right]\right\} d^{N}{r}}{\int \exp [-\beta E(r)] d^{N}{r}} \\ =&amp; P_{i}^{\mathrm{b}}(\xi) \exp \left[\beta \omega_{i}(\xi)\right] \\ &amp; \times \frac{\int \exp [-\beta E(r)] \exp \left\{-\beta \omega_{i}[\xi({r})]\right\} d^{N}{r}}{\int \exp [-\beta E(r)] d^{N}{r}} \\ =&amp; P_{i}^{\mathrm{b}}(\xi) \exp \left[\beta \omega_{i}(\xi)\right]\left\langle\exp \left[-\beta \omega_{i}(\xi)\right]\right\rangle \end{aligned}\] <p>Note that the expectation value here is the average under the equilibrium ensemble of the system. Thus, the true equilibrium free energy of the system is:</p> \[F_i (\xi) = - \frac{1}{\beta} P_{i}^{\mathrm{u}}(\xi) = -(1 / \beta) \ln P_{i}^{\mathrm{b}}(\xi)-\omega_{i}(\xi)+Q_{i}\] <p>where \(Q_i = -(1 / \beta) \ln \left\langle\exp \left[-\beta \omega_{i}(\xi)\right]\right\rangle\). We note that the true equilibrium free energy consists of three terms. The first term, \((1 / \beta) \ln P_{i}^{\mathrm{b}}(\xi)\), can be directly obtained from sampling. The second term, \(\omega_{i}(\xi)\), is the known form of the bias potential. The third term, \(Q_i\), is unknown. From its definition, we can see that \(Q_i\) actually depends on the equilibrium distribution or the equilibrium free energy itself. Therefore, such an equation usually requires a self-consistent iterative calculation to obtain.</p> <p>Notice that in the above discussion, we only considered the free energy calculation for one bias potential window. This means we only obtained the free energy \(F_i (\xi)\) for sampling in window \(i\). If we sample configurations only within one window, the sampled configurations will be concentrated near the center of window \(i\), and the probability of sampling configurations outside the window will be very low, leading to a large variance in the estimated free energy. Therefore, in practice, we almost always divide the collective variable into many bins, using multiple sampling windows to sample, obtaining an estimated value of free energy over an entire interval of the collective variable.</p> <p>Recall that in the BAR/MBAR method, when we have samples from multiple states, we choose appropriate parameters to minimize the variance of the free energy estimate. In umbrella sampling, we face a similar situation. The real problem in calculating free energy is: given multiple sampling points from different windows, how do we use these sampling points to minimize the variance of the free energy estimate? The Weighted Histogram Analysis Method (WHAM) aims to achieve this.</p> <p>First, we set the unbiased probability distribution estimate obtained from sampling in window \(i\) as \(P_i^{\mathrm{u}}(\xi)\). We assume that the unbiased probability distribution \(P^{\mathrm{u}}(\xi)\) over the entire interval of the collective variable is a linear combination of the probability distributions obtained from each window:</p> \[P^{\mathrm{u}}(\xi)=\sum_i^{\text {windows }} p_i(\xi) P_i^{\mathrm{u}}(\xi)\] <p>This assumption is reasonable as it satisfies that the probability ratio of different sampled configurations remains approximately unchanged within each window. To ensure that the probability distribution over the entire interval satisfies the normalization condition, we also need to satisfy:</p> \[\sum p_i = 1\] <p>Under this assumption, the WHAM method aims to minimize the variance of the unbiased probability distribution \(P^{\mathrm{u}}(\xi)\) obtained by this formula. More precisely, we want to select \(p_i\) to satisfy:</p> \[\frac{\partial \sigma^2\left(P^{\mathrm{u}}\right)}{\partial p_i}=0\] <p>After some tedious calculations (detailed derivation can be found in <a href="https://quantum.ch.ntu.edu.tw/ycclab/wp-content/uploads/2015/02/WHAM_1992.pdf">Shankar 1992</a>), we find that the coefficients \(p_i\) need to satisfy the following relationship:</p> \[p_i=\frac{a_i}{\sum_j a_j}, a_i(\xi)=N_i \exp \left[-\beta \omega_i(\xi)+\beta F_i\right]\] <p>where \(N_i\) is the number of sampled points in window \(i\), and the expression for \(F_i\) is:</p> \[\exp \left(-\beta F_i\right)=\int P^{\mathrm{u}}(\xi) \exp \left[-\beta w_i(\xi)\right] d \xi\] <p>Notice that the expression for \(F_i\) depends on the global probability distribution \(P^{\mathrm{u}}(\xi)\). Similar to the single-window sampling situation, a self-consistent iterative process is still required here to finally obtain our free energy estimate.</p>]]></content><author><name></name></author><category term="Physics"/><summary type="html"><![CDATA[This post describe equilibrium free energy theory.]]></summary></entry><entry><title type="html">Diffusion model in SO(3) space</title><link href="https://pkufjh.github.io//blog/2023/diffusion-so3/" rel="alternate" type="text/html" title="Diffusion model in SO(3) space"/><published>2023-08-01T00:00:00+00:00</published><updated>2023-08-01T00:00:00+00:00</updated><id>https://pkufjh.github.io//blog/2023/diffusion-so3</id><content type="html" xml:base="https://pkufjh.github.io//blog/2023/diffusion-so3/"><![CDATA[<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.</p> <h1 id="why-bother-diffusion-in-so3-space">Why bother diffusion in SO(3) space?</h1> <p>One typical way to represent the structure of proteins is to use the position of alpha carbon and the rotation matrix of each residue. So the diffusion process on the space of alpha carbon is simple Euclidean space, but the diffusion process on the rotation matrix (\(SO(3)\) space) is non-Euclidean. This note aims to provide a rigorous formulation of diffusion model on \(SO(3)\) space.</p> <h1 id="some-definitions-and-identities">Some definitions and identities</h1> <p>In this note, we do not always distinguish “group” and “representation of group”, just like physicists.</p> <p>With notation from the article, we define the basis of the lie algebra of \(SO(3)\) group and their matrix representation as follows:</p> \[\begin{equation} \begin{array}{ll} \mathbf{e}_1=\left[\begin{array}{l} 1 \\ 0 \\ 0 \\ \end{array}\right] &amp; G_1=\mathbf{e}_1^{\times}=\left[\begin{array}{ccc} 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -1 \\ 0 &amp; 1 &amp; 0 \end{array}\right] \\ \\ \mathbf{e}_2=\left[\begin{array}{l} 0 \\ 1 \\ 0 \end{array}\right] &amp; G_2=\mathbf{e}_2^{\times}=\left[\begin{array}{ccc} 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ -1 &amp; 0 &amp; 0 \end{array}\right] \\ \\ \mathbf{e}_3=\left[\begin{array}{l} 0 \\ 0 \\ 1 \end{array}\right] &amp; G_3=\mathbf{e}_3^{\times}=\left[\begin{array}{ccc} 0 &amp; -1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right] \end{array} \end{equation}\] <p>Thus we can exponentiate the lie algebra matrix representation to get the matrix representation of $SO(3)$ group, and get back by taking the logarithm of \(SO(3)\) matrix.</p> <p>The “box plus” operation is defined between a \(SO(3)\) matrix \(\Phi\) and a lie algebra vector \(\varphi\) \begin{equation} \label{eq:0} \boxplus: SO(3) \times \mathbb{R}^3 \rightarrow SO(3), \quad (\Phi, \boldsymbol{\varphi}) \mapsto \exp (\boldsymbol{\varphi}^{\times}) \circ \Phi \end{equation}</p> <p>The “box minus” operation is defined between two \(SO(3)\) matrix. \begin{equation} \boxminus: SO(3) \times SO(3) \rightarrow \mathbb{R}^3, \quad (\Phi_1, \Phi_2) \mapsto \log \left(\Phi_1 \circ \Phi_2^{-1}\right) \end{equation}</p> <p>\(\textbf{Some important identities}\) (\(\boldsymbol{v}\) is a vector in lie algebra, \(\boldsymbol{v}^{\times}\) is its matrix representation):</p> \[\begin{equation} \label{eq:1} \begin{aligned} \left(\boldsymbol{v}^{\times}\right)^T &amp; =-\boldsymbol{v}^{\times}, \\ \left(\boldsymbol{v}^{\times}\right)^2 &amp; =\boldsymbol{v} \boldsymbol{v}^T-\boldsymbol{v}^T \boldsymbol{v} \boldsymbol{I}, \\ (\Phi \boldsymbol{v})^{\times} &amp; = \Phi \boldsymbol{v}^{\times} \Phi^T \end{aligned} \end{equation}\] <p>Especially the third identities, which will play important roles in the derivation of the score function.</p> <p>With these definitions, we can define the derivative of a scalar function with argument to be \(SO(3)\) matrix, \begin{equation} \label{eq:2} \frac{\partial f*3}{\partial \Phi}=\lim *{\epsilon \rightarrow 0}\left[\begin{array}{l} \frac{f_3\left(\Phi \boxplus\left(\mathbf{e}_1 \epsilon\right)\right)-f_3(\Phi)}{\epsilon} <br/> \frac{f_3\left(\Phi \boxplus\left(\mathbf{e}_2 \epsilon\right)\right)-f_3(\Phi)}{\epsilon} <br/> \frac{f_3\left(\Phi \boxplus\left(\mathbf{e}_3 \epsilon\right)\right)-f_3(\Phi)}{\epsilon} \end{array}\right] \end{equation}</p> <p>We can represent the \(SO(3)\) matrix in 3D space using axis-angle representation, in explicit terms, it can be written as:</p> \[\begin{equation} \overleftrightarrow{R}_n(\theta)_{i, j}=n_i n_j+\cos \theta \cdot\left(\delta_{i, j}-n_i n_j\right)-\sin \theta \cdot \sum_c \epsilon_{i j k} n_k, \text { here } i, j, k=x, y, z \end{equation}\] <p>Or more simply written using lie algebra matrix: \begin{equation} \overleftrightarrow{R}_n(\theta)=\exp ( \theta \overleftrightarrow{G} \bullet n) \end{equation} where \(\theta\) is in the range \([0, \pi]\), and \(n\) represents the rotation axis.</p> <h1 id="prior-distribution-and-random-walk-in-so3-space">Prior distribution and random walk in SO(3) space</h1> <p>To understand the \(SO(3)\) group structure better, we can look at the double cover group of \(SO(3)\), the \(SU(2)\) group.</p> <p>The Quaternion representation of \(SU(2)\) group can be represented by four Pauli matrices as: \begin{equation} U=a_0 \sigma_0-\dot{\mathbb{I}}\left(a_1 \sigma_1+a_2 \sigma_2+a_3 \sigma_3\right)=a_0 \sigma_0-\dot{\mathbb{I}} \boldsymbol{a} \cdot \boldsymbol{\sigma} \end{equation} where \(\sum_i a_i^2=a_0^2+\boldsymbol{a}^2=1\).</p> <p>So the manifold of \(SU(2)\) group is a 3-sphere \(S^3\) in 4D Euclidean space.</p> <p>Another way to write the quaternion representation is to use axis and angle by \(U = \exp \left(-\dot{\mathbb{I}} \frac{\theta}{2} \boldsymbol{n} \bullet \boldsymbol{\sigma}\right) = \left(\cos \frac{\theta}{2}, \sin \frac{\theta}{2} \cdot \boldsymbol{n}\right)\). In this way of writing the \(SO(3)\) matrix can be represented as the adjoint representation of \(SU(2)\) group:</p> \[e^{-\mathrm{i} \frac{\theta}{2} \boldsymbol{n} \bullet \sigma} \cdot \sigma_a \cdot e^{\mathrm{i} \frac{\theta}{2} n \bullet \sigma}=\sum_b \sigma_b \cdot\left[\overleftrightarrow{R}_{\boldsymbol{n}}(\theta)\right]_{ba}\] <p>Notice that \(a_i\) and \(-a_i\) correspond to the same \(SO(3)\) matrix, so we have group structure isophism: \(\mathrm{SO}(3) \simeq \mathrm{SU}(2) / \mathbb{Z}_2\).</p> <p>Since the manifold of \(SU(2)\) is \(S^3\), the Haar measure of \(SU(2)\) can be represented as:</p> \[\begin{equation} \mathrm{d} \mu(U)=\delta\Bigg(\sqrt{\sum\nolimits*{i=0}^3 a_i^2}-1\Bigg) \prod*{i=0}^3 \mathrm{~d} a_i \end{equation}\] <p>On the other hand, we can parameterize the 4D coordinates as: \(\left(a_i\right)=r \cdot\left(\cos \frac{\theta}{2}, \sin \frac{\theta}{2}(\sin \vartheta \cos \phi, \sin \vartheta \sin \phi, \cos \vartheta)\right)\), so we can transform the measure in 4D coordinate space into the axis-angle space by computing the jacobian, namely</p> \[\begin{equation} \delta(r-1) \cdot\left|\frac{\partial\left(a_0, a_1, a_2, a_3\right)}{\partial(r, \theta, \vartheta, \phi)}\right| \cdot \mathrm{d} r \mathrm{~d} \theta \mathrm{d} \vartheta \mathrm{d} \phi=\frac{1}{2} \sin ^2 \frac{\theta}{2} \sin \vartheta \mathrm{d} \theta \mathrm{d} \vartheta \mathrm{d} \phi = \frac{1}{2} \sin ^2 \frac{\theta}{2} \mathrm{~d} \theta \cdot \mathrm{d}^2 \boldsymbol{n} \end{equation}\] <p>From this expression, we can know that in the axis-angle representation of the \(SU(2)\) group, the measure in angle space is proportional to \(\sin ^2 \frac{\theta}{2} = 1 - \cos \theta\), the measure in axis space is constant. Since the axis-angle representation of the \(SU(2)\) group has a two-to-one correspondence to the axis-angle representation of the \(SO(3)\) group with the same angle \(\theta\) and axis \(n\), the measure in the axis-angle representation of the \(SO(3)\) group angle space is also proportional to \(1 - \cos \theta\), and the measure in the axis space is uniform.</p> <p>From another point of view, the prior distribution in the rotation angle space is proportional to \(1 - \cos \theta\), and the prior distribution in the axis space is uniform.</p> <p>Now we consider the noise adding process in \(SO(3)\) space. Like the random walk in 3D space, we apply small random rotation matrix on an initial matrix iteratively. We generate three small angles from a gaussain distribution, and compose them with Lie algebra matrix to get the rotation matrix. To write it formally,</p> \[\begin{equation} r^{(t+1)}=\exp \left\lbrace\sum*{d=1}^3 \epsilon_d G_d\right\rbrace r^t \end{equation}\] <p>Specifically, we set the standard deviation of the gaussian noise to be 0.2. After a certain number of steps, we can reach the final matrix of this process, we compute the rotation angle of this final matrix relative to the initial matrix, we can get the overall distribution of the rotation angle, as in the following figure, the left figure is the single step random walk, whose angle distribution corresponds to the so called IGSO3 distribution with the same standard deviation \(\sigma = 0.2\).</p> \[\begin{equation} f(\omega, \sigma)=\frac{1-\cos \omega}{\pi} \sum*{l=0}^{\infty}(2 l+1) e^{-l(l+1) \sigma^2/2} \frac{\sin \left(\left(l+\frac{1}{2}\right) \omega\right)}{\sin (\omega / 2)} \end{equation}\] <p>The right figure is the ten steps random walk, whose angle distribution corresponds to the IGSO3 distribution with standard deviation \(\sigma = \sqrt{0.2^2 * 10} = 0.6325\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/output_1-480.webp 480w,/assets/img/output_1-800.webp 800w,/assets/img/output_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/output_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/output_10-480.webp 480w,/assets/img/output_10-800.webp 800w,/assets/img/output_10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/output_10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Rotation angle histogram during random walk. </div> <p>Other choice of steps also give us consistency of the angle distribution with the IGSO(3) distribution, and its standard deviation is found to be the square root of the sum of the gaussian distribution, \begin{equation} \sigma = \sqrt{ \sum_i \sigma_i^2} \end{equation} Note that in the limit of \(\sigma \rightarrow \infty\), the function \(f(\omega, \sigma)\) will approach exactly \(\frac{1-\cos \omega}{\pi}\), the prior distribution.</p> <h1 id="forward-process-in-so3-space">Forward process in SO(3) space</h1> <p>Since the marginal distribution of the random walk in \(SO(3)\) space is IGSO(3) distribution, we can simulate the forward process in \(SO(3)\) space by directly sampling angles from IGSO(3) distribution with a certain value of \(\sigma\), then sample an axis from an uniform distribution, then apply this random matrix upon the initial matrix. To write it formally: \begin{equation} r^t=e^{\theta_t \textbf{n}} r^0 \end{equation} where \(\theta_t\) is sampled from the IGSO(3) distribution with standard deviation \(\sigma_t\), and \(\textbf{n}\) is a 3D vector sampled from the uniform distribution.</p> <p>With the addition theorem for the noise adding process, the matrices corresponding the neighboring distribution can be related by: \begin{equation} r^t=e^{\beta*t \textbf{n}} r^{t-1} \end{equation} where \(\beta_t\) is sampled from the IGSO(3) distribution with standard deviation \(\sqrt{\sigma^2_t - \sigma^2*{t-1}}\), and \(\textbf{n}\) is a 3D vector sampled from the uniform distribution.</p> <h1 id="backward-process-in-so3-space">Backward process in SO(3) space</h1> <p>A remarkable result from Anderson states that, the reverse SDE equation for a forward SDE process: \begin{equation} \mathrm{d} \mathbf{x}=\mathbf{f}(\mathbf{x}, t) \mathrm{d} t+g(t) \mathrm{d} \mathbf{w} \end{equation} can be modeled as: \begin{equation} \mathrm{d} \mathbf{x}=\left[\mathbf{f}(\mathbf{x}, t)-g(t)^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right] \mathrm{d} t+g(t) \mathrm{d} \overline{\mathbf{w}} \end{equation} Note that this formulation is in Euclidean space, in \(SO(3)\) case, we have to use the formulation in Lie algebra space, then exponentiate back to \(SO(3)\) space.</p> <p>To write it more explicitly, we can write the random walk in Lie algebra space as: \begin{equation} L*{t+1} = L_t + \mathbf{\beta_t} \cdot \mathbf{G} \end{equation} where \(\mathbf{\beta_t}\) is sampled from three gaussian distribution with standard deviation \(\sqrt{\sigma^2_t - \sigma^2*{t-1}}\), \(\mathbf{G}\) is the Three Lie algebra matrix.</p> <p>Then the reverse process of this SDE in Lie algebra space is:</p> \[\begin{equation} L_t = L_{t+1} + (\sigma^2_t - \sigma^2_{t-1}) \nabla_{L^{(t)}} \log q\left(L_t \right) + \sqrt{\sigma^2_t - \sigma^2_{t-1}} \mathbf{\epsilon_t} \cdot \mathbf{G} \end{equation}\] <p>Exponentiate back to the \(SO(3)\) matrix, we have the reverse process in \(SO(3)\) space:</p> \[\begin{equation} r^{(t-1)}=\exp \left\{\left(\sigma_t^2-\sigma_{t-1}^2\right) \nabla_{r^{(t)}} \log q\left(r^{(t)}\right)+\sqrt{\sigma_t^2-\sigma_{t-1}^2} \sum_{d=1}^3 \epsilon_d G_d\right\} r^t, \end{equation}\] <p>where \(q (r^{(t)}\) is the probability density of matrix \(r^{(t)}\) of the marginal distribution in the forward process. Now it is only a matter of calculating the expression \(\nabla_{r^{(t)}} \log q\left(r^{(t)}\right)\), which is also called the score function.</p> <h1 id="score-function-in-so3-space-with-trained-model">Score function in \(SO(3)\) space with trained model</h1> <p>At this stage, if we do not impose model information in the diffusion process, the marginal distribution of \(q(r^{(t)})\) is just the IGSO(3) distribution as a function of the rotation angle, then the score function is the derivative of the function w.r.t the three Lie algebra basis matrices, namely:</p> \[\begin{equation} \nabla*r \log q\left(r^{(t)}\right) =\left.\nabla_r \omega\left( \bar{r}^{\top} r^{(t)} \right) \frac{d}{d \omega} [ \log \frac{ f\left(\omega ; \sigma_t^2\right) } { 1 - \cos \omega} ] \right|*{\omega=\omega\left(\bar{r}^{\top} r\right)} \end{equation}\] <p>Note we divide the IGSO3 distribution by the measure in \(SO(3)\) space to get the probability density.</p> <p>Now we impose model information in the diffusion process. First of all, according to the denoised score matching objective, we can use the conditional score to approximate the true score function:</p> \[\begin{equation} \nabla_r \log q\left(r^{(t)}\right) =\mathbb{E}_q\left[\nabla_{r^{(t)}} \log q\left(r^{(t)} \mid r^{(0)}\right) \mid r^{(t)}\right] \end{equation}\] <p>In real cases we do not know the distribution of the initial matrix \(r^{(0)}\) given the current observation \(r^{(t)}\). But if we have a trained model such that when observing a noised rotation matrix \(r^{(t)}\), the model will output a single structure \(\hat{r} (t)\) as the ground truth of the denoised structure. Then we can approximate the conditional score by the model prediction:</p> \[\begin{equation} \begin{aligned} \nabla_r \log q\left(r^{(t)}\right) &amp; \approx \nabla_{r^{(t)}} \log q\left(r^{(t)} \mid r^{(0)}=\hat{r}^{(0)}\right) \\ &amp; =\nabla_{r^{(t)}} \log \mathcal{I} \mathcal{G}_{S O(3)}\left(r^{(t)} ; \hat{r}^{(0)}, \sigma_t^2\right), \end{aligned} \end{equation}\] <p>Then the score function in \(SO(3)\) space can be written as:</p> \[\begin{equation} \nabla_r \log \mathcal{I} \mathcal{G}_{S O(3)}\left(r ; \hat{r}, \sigma_t^2\right)=\left.\nabla_r \omega\left(\hat{r}^{\top} r\right) \frac{d}{d \omega} [ \log \frac{ f\left(\omega ; \sigma_t^2\right) } { 1 - \cos \omega} ] \right|_{\omega=\omega\left(\hat{r}^{\top} r\right)} \end{equation}\] <p>Note we divide the IGSO3 distribution by the measure in \(SO(3)\) space.</p> <p>Now the crucial thing to compute is the derivative of the rotation angle with respect to the rotation matrix,namely</p> \[\begin{equation} \nabla_r \omega\left(\hat{r}^{\top} r\right) \end{equation}\] <p>This is exactly the kind of derivatives defined in eq(\ref{eq:2}), the derivative of a scalar function with respect to a \(SO(3)\) matrix.</p> <p>Some important \(\textbf{facts}\) before taking the derivatives is that, when doing matrix multiplication:</p> <ol> <li> <p>\(SO(3)\) matrix do not generally commute (unless they have the same rotation axis).</p> </li> <li> <p>Infinitesimal rotation matrix commute to the first order of rotation angle.</p> </li> <li> <p>A \(SO(3)\) matrix and an infinitesimal rotation matrix do not generally commute (unless they have the same rotation axis).</p> </li> </ol> <p>Now let us take the derivative, I only show the first dimension of the derivative.</p> <p>\begin{equation} \frac{\omega \left(\hat{r}^{\top}\left( r \boxplus\mathbf{e}_1 \epsilon\right)\right)-\omega (\hat{r}^{\top} r)}{\epsilon} \end{equation}</p> <p>Note that since \(SO(3)\) matrix acts on column vector, when we do matrix composition we will always \(\textbf{apply the new matrix on the left of the old matrix}\). This is why in eq (\ref{eq:0}) we apply the lie algebra exponential on the left of the original \(SO(3)\) matrix.</p> <p>We proceed with the defintion of “box plus”,</p> \[\begin{equation} \begin{aligned} \frac{\omega \left(\hat{r}^{\top}\left( r \boxplus\mathbf{e}_1 \epsilon\right)\right)-\omega (\hat{r}^{\top} r)}{\epsilon} &amp; = \frac{\omega \left(\hat{r}^{\top} (e^{\mathbf{e}_1 \epsilon } r ) \right) -\omega (\hat{r}^{\top} r)}{\epsilon} \\ &amp; = \frac{\omega \left(\hat{r}^{\top} (e^{\mathbf{e}_1 \epsilon } \hat{r} \hat{r}^{\top} r ) \right) -\omega (\hat{r}^{\top} r)}{\epsilon} \end{aligned} \end{equation}\] <p>Now using the third identity in eq(\ref{eq:1}), we have</p> \[\begin{equation} \begin{aligned} \frac{\omega \left(\hat{r}^{\top}\left( r \boxplus\mathbf{e}_1 \epsilon\right)\right)-\omega (\hat{r}^{\top} r)}{\epsilon} &amp; = \frac{\omega \left(\hat{r}^{\top} (e^{\mathbf{e}_1 \epsilon } r ) \right) -\omega (\hat{r}^{\top} r)}{\epsilon} \\ &amp; = \frac{\omega \left(\hat{r}^{\top} (e^{\mathbf{e}_1 \epsilon } \hat{r} \hat{r}^{\top} r ) \right) -\omega (\hat{r}^{\top} r)}{\epsilon} \\ &amp; = \frac{\omega \left( (e^{ \hat{r}^{\top} \mathbf{e}_1 \epsilon } \hat{r}^{\top} r ) \right) -\omega (\hat{r}^{\top} r)}{\epsilon} \end{aligned} \end{equation}\] <p>Now the crucial point is to compute the rotation angle from the composition of two rotation matrix.</p> <p>Note that in general, we have the \(\textbf{Baker–Campbell–Hausdorff formula}\):</p> \[\begin{equation} Z= \log[e^X e^Y] = X+Y+\frac{1}{2}[X, Y]+\frac{1}{12}[X,[X, Y]]-\frac{1}{12}[Y,[X, Y]]+\cdots \end{equation}\] <p>But since we are dealing with \(SO(3)\) matrix, we have simpler formula, which is called “Rodrigues rotation formula.”</p> <p>The Rodrigues vector associated with a rotation matrix can be expressed as: \begin{equation} \mathbf{g}=\hat{\mathbf{e}} \tan \frac{\theta}{2} \end{equation} where \(\hat{\mathbf{e}}\) is the rotation axis unit vector, \(\theta\) is the rotation angle.</p> <p>When we combine two rotation matrix, we can get the Rodrigues rotation formula: \begin{equation} (\mathbf{g}, \mathbf{f})=\frac{\mathbf{g}+\mathbf{f}-\mathbf{f} \times \mathbf{g}}{1-\mathbf{g} \cdot \mathbf{f}} \end{equation} The rotation angle composition is expressed as follows: \begin{equation} \cos \frac{\gamma}{2}=\cos \frac{\beta}{2} \cos \frac{\alpha}{2}-\sin \frac{\beta}{2} \sin \frac{\alpha}{2} \mathbf{B} \cdot \mathbf{A}, \end{equation}</p> <p>Now we assume that the Rodrigues vector associated with rotation matrix \(\hat{r}^{\top} r\) is \(\hat{A} \tan \frac{\alpha}{2}\), the Rodrigues vector associated with rotation matrix \(e^{ \hat{r}^{\top} \mathbf{e}_1 \epsilon }\) is \(\hat{r}^{\top} \mathbf{e}_1 \tan \frac{\epsilon}{2}\).</p> <p>We express the rotation angle \(\omega \left( (e^{ \hat{r}^{\top} \mathbf{e}_1 \epsilon } \hat{r}^{\top} r ) \right)\) by \(\gamma\). We then have the equation: \begin{equation} \cos \frac{\gamma}{2}=\cos \frac{\epsilon}{2} \cos \frac{\alpha}{2}-\sin \frac{\epsilon}{2} \sin \frac{\alpha}{2} \hat{r}^{\top} \mathbf{e}_1 \cdot \mathbf{A}, \end{equation}</p> <p>Since \(\epsilon\) is a small number, we taylor expand the equation: \begin{equation} \cos \frac{\gamma}{2}= \cos \frac{\alpha}{2}- \frac{\epsilon}{2} \sin \frac{\alpha}{2} \hat{r}^{\top} \mathbf{e}_1 \cdot \mathbf{A}, \end{equation} We express \(\gamma\) to be the perturbation of \(\alpha\), \(\gamma = \alpha + \delta \alpha\), then taylor expand \(\cos \frac{\alpha}{2}\), \begin{equation} \cos \frac{\alpha + \delta \alpha}{2} - \cos \frac{\alpha}{2} = - \sin \frac{\alpha}{2} \frac{\delta \alpha}{2} = -\frac{\epsilon}{2} \sin \frac{\alpha}{2} \hat{r}^{\top} \mathbf{e}_1 \cdot \mathbf{A}, \end{equation} So we have the expression for \(\delta \alpha\), \begin{equation} \delta \alpha = \epsilon \hat{r}^{\top} \mathbf{e}_1 \cdot \mathbf{A} \end{equation}</p> <p>So we finally get the derivative,</p> \[\begin{equation} \begin{aligned} \frac{\omega \left(\hat{r}^{\top}\left( r \boxplus\mathbf{e}_1 \epsilon\right)\right)-\omega (\hat{r}^{\top} r)}{\epsilon} &amp; = \frac{\hat{r}^{\top}_{xx} A_x + \hat{r}^{\top}_{yx} A_y + \hat{r}^{\top}_{zx} A_z}{\sqrt{A_x^2 + A_y^2 +A_z^2 }} \\ &amp; = \frac{\hat{r}_{xx} A_x + \hat{r}_{xy} A_y + \hat{r}_{xz} A_z}{\sqrt{A_x^2 + A_y^2 +A_z^2 }} \end{aligned} \end{equation}\] <p>Similarly we get the derivative in other two directions,</p> \[\begin{equation} \frac{\omega \left(\hat{r}^{\top}\left( r \boxplus\mathbf{e}_2 \epsilon\right)\right)-\omega (\hat{r}^{\top} r)}{\epsilon} = \frac{\hat{r}_{yx} A_x + \hat{r}_{yy} A_y + \hat{r}_{yz} A_z}{\sqrt{A_x^2 + A_y^2 +A_z^2 }} \end{equation}\] \[\begin{equation} \frac{\omega \left(\hat{r}^{\top}\left( r \boxplus\mathbf{e}_3 \epsilon\right)\right)-\omega (\hat{r}^{\top} r)}{\epsilon} = \frac{\hat{r}_{zx} A_x + \hat{r}_{zy} A_y + \hat{r}_{zz} A_z}{\sqrt{A_x^2 + A_y^2 +A_z^2 }} \end{equation}\] <p>Write in a compact form, using the identity \begin{equation} \hat{r} v = \log (\hat{r} e^v \hat{r}^{\top}) \end{equation} \begin{equation} \nabla_r \omega\left(\hat{r}^{\top} r\right) = \frac{\hat{r} A}{\omega(\hat{r}^{\top} r)} = \frac{\hat{r} \log(\hat{r}^{\top} r)}{\omega(\hat{r}^{\top} r)} = \frac{ \log(r \hat{r}^{\top})}{\omega(\hat{r}^{\top} r)} = \frac{r \log(\hat{r}^{\top} r)}{\omega(\hat{r}^{\top} r)} \end{equation} Note that the result is in the lie algebra space, which in \(\mathbb{R}^3\).</p> <p>To use this score correctly, we need to first transform the vector into matrix representation. Then exponentiate it.</p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>]]></content><author><name></name></author><category term="AI"/><summary type="html"><![CDATA[This post describe the diffusion model formulation in SO(3) space.]]></summary></entry><entry><title type="html">How to get the totp code</title><link href="https://pkufjh.github.io//blog/2022/totp/" rel="alternate" type="text/html" title="How to get the totp code"/><published>2022-11-01T00:00:00+00:00</published><updated>2022-11-01T00:00:00+00:00</updated><id>https://pkufjh.github.io//blog/2022/totp</id><content type="html" xml:base="https://pkufjh.github.io//blog/2022/totp/"><![CDATA[<ol> <li>Get the TOTP QR code.</li> <li>Install zbar, decode the QR code.</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>zbar
zbarimg qr.png
</code></pre></div></div> <ol> <li>Generate totp code from the decoded code.</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from dpdispatcher.utils import generate_totp
print<span class="o">(</span>generate_totp<span class="o">(</span><span class="s2">"A5UKDJDSDSK"</span><span class="o">))</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="Tech"/><summary type="html"><![CDATA[This post describe the process to get the totp code from QR code.]]></summary></entry><entry><title type="html">Constrained MD mean force estimator</title><link href="https://pkufjh.github.io//blog/2022/constrained-md/" rel="alternate" type="text/html" title="Constrained MD mean force estimator"/><published>2022-10-01T00:00:00+00:00</published><updated>2022-10-01T00:00:00+00:00</updated><id>https://pkufjh.github.io//blog/2022/constrained-md</id><content type="html" xml:base="https://pkufjh.github.io//blog/2022/constrained-md/"><![CDATA[<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.</p> <h1 id="constrained-md-formulation">Constrained MD formulation</h1> <p>The Lagrangian of the system \(\mathcal{L}\) is extended as:</p> \[\begin{equation} \mathcal{L}^*(\mathbf{q}, \dot{\mathbf{q}})=\mathcal{L}(\mathbf{q}, \dot{\mathbf{q}})+\sum_{i=1}^r \lambda_i \sigma_i(q) \end{equation}\] <p>where the summation is over \(r\) geometric constraints, \(\mathcal{L}^*\) is the Lagrangian for the extended system, and \(\lambda_i\) is a Lagrange multiplier associated with a geometric constraint \(\sigma_i\):</p> \[\begin{equation} \sigma_i(q)=\xi_i(q)-\xi_i \end{equation}\] <p>with \(\xi_i(q)\) being a geometric parameter and \(\xi_i\) is the value of \(\xi_i(q)\) fixed during the simulation.</p> <p>In the SHAKE algorithm, the Lagrange multipliers \(\lambda_i\) are determined in the iterative procedure:</p> <ol> <li>Perform a standard MD step (leap-frog algorithm): \(\begin{equation} \begin{aligned} v_i^{t+\Delta t / 2} &amp; =v_i^{t-\Delta t / 2}+\frac{a_i^t}{m_i} \Delta t \\ q_i^{t+\Delta t} &amp; =q_i^t+v_i^{t+\Delta t / 2} \Delta t \end{aligned} \end{equation}\)</li> <li>Use the new positions \(q^{t+\Delta t}\) to compute Lagrange multipliers for all constraints: \(\begin{equation} \lambda_k=\frac{1}{\Delta t^2} \frac{\sigma_k\left(q^{t+\Delta t}\right)}{\sum_{i=1}^N m_i^{-1} \nabla_i \sigma_k\left(q^t\right) \nabla_i \sigma_k\left(q^{t+\Delta t}\right)} \end{equation}\)</li> <li>Update the velocities and positions by adding a contribution due to restoring forces (proportional to \(\lambda_k\)): \(\begin{equation} \begin{aligned} &amp; v_i^{t+\Delta t / 2}=v_i^{t-\Delta t / 2}+\left(a_i^t-\sum_k \frac{\lambda_k}{m_i} \nabla_i \sigma_k\left(q^t\right)\right) \Delta t \\ &amp; q_i^{t+\Delta t}=q_i^t+v_i^{t+\Delta t / 2} \Delta t \end{aligned} \end{equation}\)</li> <li>repeat steps 2-4 until either \(\mid \sigma_i(q) \mid\) are smaller than a predefined tolerance, or the number of iterations exceeds a predefined threshold.</li> </ol> <h1 id="free-energy-and-mean-force-definition">Free energy and mean force definition</h1> <p>The definition for free energy w.r.t some predefined collective varibales (CVs) is:</p> \[\begin{equation} A(s)=-k_b \operatorname{Tln} p(s)=-\frac{1}{\beta} \ln \int \frac{1}{Z} e^{-\beta U(r)} \Pi_i \delta\left(s_i(r)-s_i\right) d r \end{equation}\] <p>Accordingly, the mean force is the negative gradient of the free energy:</p> \[\begin{equation} - \nabla_{s_i} A(s)= - \frac{1}{\beta Z} \int e^{-\beta U(r)} \nabla_{s_i} \Pi_j \delta\left(s_j(r)-s_j\right) d r \end{equation}\] <h1 id="mean-force-estimator-for-constrained-md">Mean force estimator for constrained MD</h1> <p>We show that if there exists \(b_i(r)\), such that:</p> \[\begin{equation} \label{eq:condition} \nabla_r s_i(r) \cdot b_j(r)=\delta_{i j} \end{equation}\] <p>then the mean force estimator expression is:</p> \[\begin{equation} -\nabla_{s_i} A(s)=\frac{1}{Z} \int e^{-\beta U(r)}\left(-\nabla U(r) \cdot b_i(r)+\frac{1}{\beta} \nabla \cdot b_i(r)\right) \Pi_j \delta\left(s_j(r)-s_j\right) d r \end{equation}\] <p>The proof is in the following, from Eq. (\ref{eq:condition}) we can get:</p> \[\begin{equation} b_j(r) \cdot \nabla_r \delta\left(s_i(r)-s_i\right)=b_j(r) \cdot \nabla_r\left(s_i(r)\right) \cdot \nabla_{s_i} \delta\left(s_i(r)-s_i\right)=\delta_{i j} \cdot \nabla_{s i} \delta\left(s_i(r)-s_i\right) \end{equation}\] <p>Then:</p> \[\begin{equation} b_i(r) \cdot \nabla_r \Pi_j \delta\left(s_j(r)-s_j\right)=\nabla_{s_i} \delta\left(s_i(r)-s_i\right) \Pi_{j \neq I} \delta\left(s_j(r)-s_j\right) \end{equation}\] <p>So:</p> \[\begin{equation} \begin{aligned} \nabla_{s_i} A(s)&amp; =\frac{1}{\beta Z} \int e^{-\beta U(r)} \nabla_{s_i} \Pi_j \delta\left(s_j(r)-s_j\right) d r=\frac{1}{\beta Z} \int e^{-\beta U(r)} b_i(r) \nabla_r \Pi_j \delta\left(s_j(r)- s_j\right) d r \\ &amp; =\frac{1}{\beta Z} \int e^{-\beta U(r)}\left(\beta \nabla U(r) \cdot b_i(r)-\nabla \cdot b_i(r)\right) \Pi_j \delta\left(s_j(r)-s_j\right) d r \\ &amp; =&lt;\nabla U(r) \cdot b_i(r)-\frac{1}{\beta} \nabla \cdot b_i(r)&gt;_{\text {cond }} \end{aligned} \end{equation}\] <p>So:</p> \[\begin{equation} \begin{aligned} &amp; f_i=-\nabla_{s_i} A(s)=&lt;-\nabla U(r) \cdot b_i(r)+\frac{1}{\beta} \nabla \cdot b_i(r)&gt;_{\text {cond }}=&lt;f(r) \cdot b_i(r)+\frac{1}{\beta} \nabla b_i(r)&gt;_{\text {cond }} \end{aligned} \end{equation}\] <p>This mean force average, like any other observables, however, is not the same with the “average” taken from a “constrained md” trajectory. A one sentence explanation is: The dynamics of constrianed MD is non-Hamiltonian, namely the equation of motion of constrained MD does not preserve volume element due to the non-zero “compressibility”, so we have to take a factor into the “conditional average”, to get the “constrained MD average”.</p> <h1 id="constrained-ensemble">Constrained ensemble</h1> <p>To see this more clearly, we have to go back to the definition of constrained MD. The equation of motion for constrained MD is,</p> \[\begin{equation} \begin{aligned} \dot{x_j} &amp; =M^{-1} p_j \\ \dot{p_j} &amp; =-\frac{\partial V}{\partial x_j}+\frac{\partial \sigma}{\partial x_j} \lambda(x, p) \end{aligned} \end{equation}\] <p>The mass matrix is diagonal in general, so I do not write lower index. From theoretical mechanics, it is easy to see that the evolution of the jacabian in phase space is,</p> \[\begin{equation} \frac{d J}{d t}=J\left(\partial_j \dot{\xi}^j\right) \end{equation}\] <p>So if we define “compressibility” to be:</p> \[\begin{equation} \kappa=\partial_j \dot{\xi}^j \end{equation}\] <p>Then the evolution of the jacobian is:</p> \[\begin{equation} J(\xi(t) ; \xi(0))=\exp \left(\int_0^t d s \kappa(\xi(s))\right)=\exp (W(\xi(t))-W(\xi(0)) \end{equation}\] <p>If we define a “metric factor”:</p> \[\begin{equation} \sqrt{g(t)} \equiv e^{-W(\xi(t))} \end{equation}\] <p>Then we can still get a constant “volume element”:</p> \[\begin{equation} \sqrt{g(t)} d^{2 n} \xi(t)=\sqrt{g(0)} d^{2 n} \xi(0) \end{equation}\] <p>This is equivalent to bring the metric factor into the “conditonal average”, namely,</p> \[\begin{equation} \langle\mathcal{O}(\mathbf{r})\rangle_s^{\text {constr }}=\frac{\int \mathrm{d} \mathbf{r} \mathrm{e}^{-\beta U(\mathbf{r})} z^{1 / 2}(\mathbf{r}) \mathcal{O}(\mathbf{r}) \delta\left(f_1(\mathbf{r})-s\right)}{\left\langle z^{1 / 2}(\mathbf{r}) \delta\left(f_1(\mathbf{r})-s\right)\right\rangle} \end{equation}\] <p>Rememble the “conditional average” is,</p> \[\begin{equation} \langle\mathcal{O}(\mathbf{r})\rangle_s^{\text {cond }}=\frac{\int \mathrm{dre}^{-\beta U(\mathbf{r})} \mathcal{O}(\mathbf{r}) \delta\left(f_1(\mathbf{r})-s\right)}{\left\langle\delta\left(f_1(\mathbf{r})-s\right)\right\rangle} \end{equation}\] <p>So if we would like to use the trajectory from constrained MD to estimate mean force in the conditional average expression, we have to bring a metrix factor in, namely,</p> \[\begin{equation} \langle\mathcal{O}(\mathbf{r})\rangle_s^{\text {cond }}=\frac{\left\langle z^{-1 / 2}(\mathbf{r}) \mathcal{O}(\mathbf{r})\right\rangle_s^{\text {constr }}}{\left\langle z^{-1 / 2}(\mathbf{r})\right\rangle_s^{\text {constr }}} \end{equation}\] <h1 id="phase-factor">Phase factor</h1> <p>Then it is a matter of calculating the phase factor (metric factor ) in constrained MD. From the analysis of the previous section, to calculate the metric factor we need to calculate,</p> \[\begin{equation} \partial_j \dot{\xi}^j \text {, where } \xi^j=\left(q^j, p^j\right) \text {. } \end{equation}\] <p>From the equation of motion, this amounts to calculate \(\partial_{p_i} \frac{\partial \sigma}{\partial x_i} \lambda (x,p)\) I first give the answer and will write the derivation later. The phase factor equals:</p> \[\begin{equation} z^{1 / 2}=\left(\operatorname{det}\left[\frac{\partial \sigma_m}{\partial x_j} M^{-1} \frac{\partial \sigma_n}{\partial x_j}\right]\right)^{1 / 2} \end{equation}\] <p>Where \(\sigma_m\) is the mth constraints exerted on the system.</p>]]></content><author><name></name></author><category term="Physics"/><summary type="html"><![CDATA[This post describe the formulation of the constrained MD mean force estimator.]]></summary></entry></feed>